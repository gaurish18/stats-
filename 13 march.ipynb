{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841449b2-d166-411c-9ab1-3812833c4733",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical technique used to compare means across multiple groups. However, for ANOVA results to be valid, certain assumptions must be met. The main assumptions for ANOVA are:\n",
    "\n",
    "1. **Independence of Observations:**\n",
    "   - **Assumption:** Observations within each group are independent of each other.\n",
    "   - **Violation Example:** If observations within a group are correlated, it can lead to inflated significance levels and unreliable results. For instance, if repeated measurements are taken on the same subjects over time and these measurements are not independent, it can violate this assumption.\n",
    "\n",
    "2. **Homogeneity of Variances (Homoscedasticity):**\n",
    "   - **Assumption:** The variance (spread) of the scores in each group should be roughly the same.\n",
    "   - **Violation Example:** Heteroscedasticity occurs when the variances are not equal across groups. For example, if one group has much larger variability than another, it can lead to inaccurate conclusions about the equality of means.\n",
    "\n",
    "3. **Normality of Residuals:**\n",
    "   - **Assumption:** The residuals (the differences between observed and predicted values) should be approximately normally distributed.\n",
    "   - **Violation Example:** If the residuals are not normally distributed, it can affect the accuracy of p-values and confidence intervals. This assumption is less critical for larger sample sizes due to the Central Limit Theorem, but severe departures from normality can still impact the results.\n",
    "\n",
    "4. **Homogeneity of Regression Slopes (for ANOVA with Covariates):**\n",
    "   - **Assumption:** If there are covariates, the relationship between the covariates and the dependent variable should be consistent across groups.\n",
    "   - **Violation Example:** If the interaction between a covariate and the grouping variable is significant, it suggests that the slopes of the regression lines are different, violating this assumption.\n",
    "\n",
    "5. **Random Sampling (for One-Way ANOVA):**\n",
    "   - **Assumption:** The data should be collected through a random sampling process.\n",
    "   - **Violation Example:** If the sampling process is not random, there may be biases in the sample, and the results may not generalize well to the population.\n",
    "\n",
    "It's important to note that ANOVA is robust, meaning it can tolerate violations to some extent, especially with larger sample sizes. However, serious violations can still compromise the validity of the results. In such cases, alternative non-parametric tests or transformations of the data may be considered. Researchers should always check the assumptions and, if violated, interpret the results with caution or explore alternative analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7092488-1671-4763-adfa-658909e12a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634d1a7b-a085-473c-b12a-6bafb2f6e475",
   "metadata": {},
   "source": [
    "There are three main types of Analysis of Variance (ANOVA), each designed to handle different experimental designs and situations:\n",
    "\n",
    "1. **One-Way ANOVA:**\n",
    "   - **Situation:** Used when there is one independent variable with more than two levels or groups.\n",
    "   - **Example:** Suppose you are testing the effectiveness of three different teaching methods (A, B, and C) on student performance. The independent variable is the teaching method, and the dependent variable is the student performance score. One-way ANOVA can be used to determine if there are any statistically significant differences in mean performance scores across the three teaching methods.\n",
    "\n",
    "2. **Two-Way ANOVA:**\n",
    "   - **Situation:** Used when there are two independent variables.\n",
    "   - **Example:** Imagine a study where you are investigating the effects of both a drug treatment and gender on blood pressure. Here, the independent variables are the drug treatment (A or B) and gender (Male or Female), and the dependent variable is blood pressure. Two-way ANOVA can help assess whether there are significant main effects and interactions between the two independent variables.\n",
    "\n",
    "3. **Repeated Measures ANOVA:**\n",
    "   - **Situation:** Used when the same subjects are used for each treatment (repeated measurements) or when there are matched pairs.\n",
    "   - **Example:** Suppose you are conducting a study to examine the effect of a new drug on blood pressure, and each participant's blood pressure is measured before and after the drug treatment. Repeated Measures ANOVA can be applied to determine if there is a significant difference in blood pressure over time due to the drug treatment.\n",
    "\n",
    "These three types of ANOVA are extensions of the basic ANOVA framework and are chosen based on the design of the study. Researchers select the appropriate ANOVA method based on the number of independent variables, the nature of the experimental design, and the characteristics of the data. It's crucial to match the statistical analysis to the specific structure of the data to obtain accurate and meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9a0f-7c3b-41bd-b753-5de45b0873fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "573d911c-fe4f-4262-b6b9-25054f256b01",
   "metadata": {},
   "source": [
    "The partitioning of variance in Analysis of Variance (ANOVA) refers to the division of the total variance in the data into different components, each associated with a specific source or factor. Understanding this concept is crucial for interpreting the results of ANOVA and gaining insights into the variability in the data. The partitioning is typically represented as follows:\n",
    "\n",
    "\\[ \\text{Total Variance} = \\text{Between-Group Variance} + \\text{Within-Group Variance} \\]\n",
    "\n",
    "Here's a breakdown of the components:\n",
    "\n",
    "1. **Total Variance:**\n",
    "   - This represents the overall variability in the data. It is the sum of the variances within each group plus the variance between the group means.\n",
    "\n",
    "2. **Between-Group Variance:**\n",
    "   - This component reflects the variability in the means of the different groups. It measures how much the group means differ from each other. A larger between-group variance suggests that there are significant differences in the group means.\n",
    "\n",
    "3. **Within-Group Variance (or Error Variance):**\n",
    "   - This component accounts for the variability within each group. It represents the differences between individual observations and their respective group means. A smaller within-group variance indicates that the observations within each group are relatively homogeneous.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "- **Identification of Group Differences:**\n",
    "  - The between-group variance helps determine whether the means of the groups are significantly different from each other. If the between-group variance is large relative to the within-group variance, it suggests that the group means are distinct.\n",
    "\n",
    "- **Calculation of F-statistic:**\n",
    "  - The F-statistic, calculated as the ratio of the between-group variance to the within-group variance, is the test statistic used in ANOVA. A larger F-statistic indicates greater evidence against the null hypothesis of equal group means.\n",
    "\n",
    "- **Interpretation of Results:**\n",
    "  - Knowing how much of the total variance is due to differences between groups versus within groups provides insight into the overall pattern of variability in the data. This information is crucial for drawing meaningful conclusions about the factors that may influence the dependent variable.\n",
    "\n",
    "- **Effect Size:**\n",
    "  - The partitioning of variance allows researchers to calculate effect sizes, such as eta-squared or omega-squared, which provide a measure of the proportion of variance in the dependent variable explained by the independent variable(s).\n",
    "\n",
    "In summary, understanding the partitioning of variance in ANOVA helps researchers assess the significance of group differences, interpret the F-statistic, and gain a nuanced understanding of the sources of variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab406d98-e64f-4503-bbab-e7f7b718733a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e47718-a92c-4306-9783-b04a269130c6",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) can be calculated using the following formulas:\n",
    "\n",
    "1. **Total Sum of Squares (SST):**\n",
    "   \\[ SST = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X})^2 \\]\n",
    "   where \\(k\\) is the number of groups, \\(n_i\\) is the number of observations in group \\(i\\), \\(X_{ij}\\) is the \\(j\\)-th observation in group \\(i\\), and \\(\\bar{X}\\) is the overall mean.\n",
    "\n",
    "2. **Explained Sum of Squares (SSE):**\n",
    "   \\[ SSE = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2 \\]\n",
    "   where \\(n_i\\) is the number of observations in group \\(i\\), \\(\\bar{X}_i\\) is the mean of group \\(i\\), and \\(\\bar{X}\\) is the overall mean.\n",
    "\n",
    "3. **Residual Sum of Squares (SSR):**\n",
    "   \\[ SSR = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2 \\]\n",
    "   where \\(k\\) is the number of groups, \\(n_i\\) is the number of observations in group \\(i\\), \\(X_{ij}\\) is the \\(j\\)-th observation in group \\(i\\), and \\(\\bar{X}_i\\) is the mean of group \\(i\\).\n",
    "\n",
    "Here's an example of how you can calculate these sums of squares in Python using the NumPy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c42f67-8fbc-44af-9a17-cccb6bf3f14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1080.4\n",
      "Explained Sum of Squares (SSE): 1000.0\n",
      "Residual Sum of Squares (SSR): 80.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "group1 = np.array([10, 12, 15, 8, 11])\n",
    "group2 = np.array([20, 22, 18, 25, 21])\n",
    "group3 = np.array([30, 28, 35, 32, 31])\n",
    "\n",
    "# Combine data from all groups\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate SST\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate SSE\n",
    "sse = np.sum([len(group) * (np.mean(group) - overall_mean)**2 for group in [group1, group2, group3]])\n",
    "\n",
    "# Calculate SSR\n",
    "ssr = np.sum([(x - np.mean(group))**2 for group in [group1, group2, group3] for x in group])\n",
    "\n",
    "# Display results\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c8463-01bc-44c3-a2be-efc9e3dff098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7af7407a-f448-489f-88ac-ee98042e28b0",
   "metadata": {},
   "source": [
    "In a two-way ANOVA, you can calculate the main effects and interaction effects for each factor using Python. The main effects represent the individual influences of each independent variable, while the interaction effect assesses whether the combined effect of the two variables is different from the sum of their individual effects.\n",
    "\n",
    "Here's an example using Python and the scipy.stats module for a two-way ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0a3217-5650-418f-b56a-3fab6cd7fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall ANOVA results:\n",
      "F-statistic: 2.633590504451039\n",
      "p-value: 0.18036517733442378\n",
      "\n",
      "Main Effects:\n",
      "Main Effect of A: -8.75\n",
      "Main Effect of B: -4.0\n",
      "\n",
      "Interaction Effect:\n",
      "Interaction Effect (A*B): 15.383590504451039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {'A': np.random.randint(1, 10, 20),\n",
    "        'B': np.random.randint(1, 10, 20),\n",
    "        'Value': np.random.randint(50, 100, 20)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "result = df.groupby(['A', 'B'])['Value'].apply(list).reset_index()\n",
    "anova_result = f_oneway(*result['Value'].to_list())\n",
    "\n",
    "# Display the overall ANOVA results\n",
    "print(\"Overall ANOVA results:\")\n",
    "print(\"F-statistic:\", anova_result.statistic)\n",
    "print(\"p-value:\", anova_result.pvalue)\n",
    "\n",
    "# Calculate main effects\n",
    "main_effect_A = df.groupby('A')['Value'].mean().diff().iloc[1]\n",
    "main_effect_B = df.groupby('B')['Value'].mean().diff().iloc[1]\n",
    "\n",
    "print(\"\\nMain Effects:\")\n",
    "print(\"Main Effect of A:\", main_effect_A)\n",
    "print(\"Main Effect of B:\", main_effect_B)\n",
    "\n",
    "# Calculate interaction effect\n",
    "interaction_effect = anova_result.statistic - main_effect_A - main_effect_B\n",
    "\n",
    "print(\"\\nInteraction Effect:\")\n",
    "print(\"Interaction Effect (A*B):\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781adb42-f004-44d6-88f7-9bd1484a4734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62b3e56f-8968-44bf-bd2d-6d3950c7cd54",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are significant differences among the means of the groups. The p-value associated with the F-statistic helps determine the statistical significance of these differences. Here's how you can interpret the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3609b-fe87-4855-9315-bf3ac08be455",
   "metadata": {},
   "source": [
    "Null Hypothesis (H₀): The null hypothesis in a one-way ANOVA asserts that there are no significant differences among the group means. Mathematically, it can be stated as H0:μ1=μ2=…=μk\t, where μi is the mean of group of i.\n",
    "\n",
    "Alternative Hypothesis (H₁): The alternative hypothesis is that at least one group mean is different from the others: \n",
    "At least one μi is different.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709d1a5-01c0-445e-9b46-0e4b788b7a8c",
   "metadata": {},
   "source": [
    "Based on the F-statistic and p-value:\n",
    "\n",
    "F-Statistic (5.23): This is a ratio of the variability between group means to the variability within groups. A larger F-statistic suggests more evidence against the null hypothesis.\n",
    "\n",
    "P-value (0.02): This is the probability of obtaining an F-statistic as extreme as the one observed if the null hypothesis were true. A smaller p-value indicates stronger evidence against the null hypothesis.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "P-value < Significance Level (e.g., 0.05): In this case, the p-value (0.02) is less than the typical significance level of 0.05. Therefore, you would reject the null hypothesis.\n",
    "\n",
    "Conclusion: There is enough evidence to conclude that there are significant differences among the group means. In other words, at least one group mean is different from the others.\n",
    "\n",
    "Practical Significance: While the differences are statistically significant, it's also important to consider whether these differences are practically significant or meaningful in the context of the study.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02, you would reject the null hypothesis and conclude that there are significant differences among the groups. It suggests that the factor being studied (the one-way grouping variable) has a statistically significant effect on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e6b8f-2824-43fb-b648-b15cfe8056d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12c8aac9-75cb-44fe-aed2-27f3b5c7e2a4",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is important to ensure that the analysis is based on as complete and unbiased information as possible. The approach to dealing with missing data can have consequences on the validity and reliability of the results. Here are common methods for handling missing data in repeated measures ANOVA and their potential consequences:\n",
    "\n",
    "1. **Complete Case Analysis (Listwise Deletion):**\n",
    "   - **Method:** Exclude cases with missing data on any variable involved in the analysis.\n",
    "   - **Consequences:**\n",
    "     - Reduces sample size, potentially leading to a loss of statistical power.\n",
    "     - May introduce bias if the missing data is not missing completely at random (MCAR).\n",
    "\n",
    "2. **Pairwise Deletion:**\n",
    "   - **Method:** Analyze all available data for each pairwise comparison.\n",
    "   - **Consequences:**\n",
    "     - Preserves more data than listwise deletion, but can lead to different sample sizes for different comparisons.\n",
    "     - Results may be difficult to interpret if the missing data pattern is not random.\n",
    "\n",
    "3. **Imputation Methods:**\n",
    "   - **Methods:** Impute missing values with estimated values based on observed data (e.g., mean imputation, regression imputation).\n",
    "   - **Consequences:**\n",
    "     - Preserves sample size but may introduce bias if the imputation model is misspecified.\n",
    "     - Underestimates standard errors, leading to overly optimistic p-values and confidence intervals.\n",
    "\n",
    "4. **Mixed Models (Longitudinal Data Analysis):**\n",
    "   - **Method:** Analyze data using mixed-effects models, which can handle missing data under the missing at random (MAR) assumption.\n",
    "   - **Consequences:**\n",
    "     - Preserves sample size and provides unbiased estimates under the MAR assumption.\n",
    "     - Requires the assumption that missing data is related to observed data, given the observed data.\n",
    "\n",
    "5. **Multiple Imputation:**\n",
    "   - **Method:** Generate multiple datasets with imputed values, perform analyses on each dataset, and combine results.\n",
    "   - **Consequences:**\n",
    "     - Accounts for uncertainty due to missing data and provides more accurate standard errors and confidence intervals.\n",
    "     - Assumes missing data is missing at random (MAR) and requires careful imputation model specification.\n",
    "\n",
    "**Potential Consequences of Using Different Methods:**\n",
    "\n",
    "- **Bias:** Inappropriately handling missing data can introduce bias into the analysis, leading to inaccurate parameter estimates and incorrect conclusions.\n",
    "\n",
    "- **Loss of Power:** Methods that result in a reduction of sample size (e.g., listwise deletion) may reduce the statistical power of the analysis, making it more challenging to detect true effects.\n",
    "\n",
    "- **Invalid Inferences:** Inaccurate handling of missing data can lead to invalid statistical inferences, including incorrect p-values and confidence intervals.\n",
    "\n",
    "- **Model Assumptions:** Different methods may rely on different assumptions about the missing data mechanism (MCAR, MAR). Choosing an inappropriate method can violate these assumptions.\n",
    "\n",
    "When dealing with missing data in a repeated measures ANOVA, researchers should carefully consider the nature of the missing data and select a method that aligns with the assumptions and goals of the analysis. Multiple imputation and mixed-effects models are often recommended for their ability to provide unbiased estimates and account for uncertainty associated with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26a777-6b5a-4a58-bcf5-0c1d8f26fd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f841fa48-8e01-41aa-b036-fa752a4cdd2b",
   "metadata": {},
   "source": [
    "After conducting an Analysis of Variance (ANOVA) and finding a significant overall effect, post-hoc tests are often performed to determine which specific groups differ from each other. Here are some common post-hoc tests, along with situations where each might be appropriate:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD):**\n",
    "   - **Use:** Tukey's HSD is widely used when there are multiple group comparisons. It controls the familywise error rate and is suitable for situations where there are more than two groups.\n",
    "   - **Example:** In a study comparing the effectiveness of three different teaching methods (A, B, and C) on student performance, if the overall ANOVA indicates a significant difference, Tukey's HSD can be used to identify which pairs of methods have significantly different means.\n",
    "\n",
    "2. **Bonferroni Correction:**\n",
    "   - **Use:** Bonferroni correction is a conservative method to control the familywise error rate by adjusting the significance level. It is appropriate when conducting multiple comparisons.\n",
    "   - **Example:** Suppose you are comparing the means of four different treatment groups. If the overall ANOVA is significant, Bonferroni correction can be applied to compare each pair of groups while controlling for the increased risk of Type I errors associated with multiple comparisons.\n",
    "\n",
    "3. **Sidak Correction:**\n",
    "   - **Use:** Similar to Bonferroni, Sidak correction is another method to control the familywise error rate. It is often less conservative than Bonferroni and may be preferred in some situations.\n",
    "   - **Example:** In a clinical trial with multiple treatment groups, if the ANOVA suggests a significant difference, Sidak correction can be used for pairwise comparisons to maintain control over the overall Type I error rate.\n",
    "\n",
    "4. **Dunnett's Test:**\n",
    "   - **Use:** Dunnett's test is appropriate when there is a control group, and you are interested in comparing each treatment group to the control group while controlling the overall Type I error rate.\n",
    "   - **Example:** In a drug efficacy study where a control group receives a placebo and multiple experimental groups receive different doses of a drug, Dunnett's test can be used to compare each experimental group to the control group.\n",
    "\n",
    "5. **Games-Howell Test:**\n",
    "   - **Use:** The Games-Howell test is a robust post-hoc test that does not assume equal variances or sample sizes. It is suitable when these assumptions are violated.\n",
    "   - **Example:** If the variances across groups are unequal, and the sample sizes are different, the Games-Howell test can be employed for pairwise comparisons following a significant ANOVA result.\n",
    "\n",
    "**Example Situation:**\n",
    "Consider a scenario where a researcher is investigating the impact of three different training programs on employees' productivity. After conducting a one-way ANOVA, if the result indicates a significant difference in productivity across the training programs, the researcher might use Tukey's HSD or another appropriate post-hoc test to identify which specific pairs of training programs are significantly different from each other. This helps in understanding the nuances of the differences among the training programs rather than just knowing that there is a significant overall difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf203a1e-7b7a-48d1-9276-8ed7c4b3af99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22df92fc-bcf4-407f-af99-afaa1a7b34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA Results:\n",
      "F-statistic: 7.467923640553487\n",
      "p-value: 0.0008149177088645907\n",
      "\n",
      "Reject the null hypothesis.\n",
      "There is sufficient evidence to suggest that there are significant differences in the mean weight loss among the three diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Generating random data for demonstration (replace with your actual data)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "weight_loss_A = np.random.normal(5, 2, 50)  # mean=5, std=2\n",
    "weight_loss_B = np.random.normal(4.5, 1.8, 50)  # mean=4.5, std=1.8\n",
    "weight_loss_C = np.random.normal(6, 2.5, 50)  # mean=6, std=2.5\n",
    "\n",
    "# Combine data from all groups\n",
    "all_weight_loss = np.concatenate([weight_loss_A, weight_loss_B, weight_loss_C])\n",
    "\n",
    "# Create a grouping variable\n",
    "group_labels = ['A'] * 50 + ['B'] * 50 + ['C'] * 50\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "result = f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Display the results\n",
    "print(\"One-way ANOVA Results:\")\n",
    "print(\"F-statistic:\", result.statistic)\n",
    "print(\"p-value:\", result.pvalue)\n",
    "\n",
    "# Interpret the results\n",
    "if result.pvalue < 0.05:\n",
    "    print(\"\\nReject the null hypothesis.\")\n",
    "    print(\"There is sufficient evidence to suggest that there are significant differences\"\n",
    "          \" in the mean weight loss among the three diets.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject the null hypothesis.\")\n",
    "    print(\"There is not enough evidence to suggest significant differences\"\n",
    "          \" in the mean weight loss among the three diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101b79d-e54d-4864-b15f-efee4e52d64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b707d9-6b36-432b-b761-7c6f0a17a1f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, size\u001b[38;5;241m=\u001b[39mn \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrograms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprograms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExperience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTimeTaken\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_taken\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform two-way ANOVA\u001b[39;00m\n\u001b[1;32m     17\u001b[0m formula \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeTaken ~ Programs + Experience + Programs:Experience\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:663\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    657\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    658\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generating random data for demonstration (replace with your actual data)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "n = 30  # number of employees per group\n",
    "programs = np.repeat(['A', 'B', 'C'], n)\n",
    "experience = np.tile(['Novice', 'Experienced'], n * 3)\n",
    "time_taken = np.random.normal(loc=20, scale=5, size=n * 3)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Programs': programs, 'Experience': experience, 'TimeTaken': time_taken})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "formula = 'TimeTaken ~ Programs + Experience + Programs:Experience'\n",
    "model = ols(formula, data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Display the ANOVA table\n",
    "print(\"Two-way ANOVA Results:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "print(\"\\nInterpretation:\")\n",
    "if anova_table['PR(>F)']['Programs'] < alpha:\n",
    "    print(\"There is a significant main effect of Programs.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of Programs.\")\n",
    "\n",
    "if anova_table['PR(>F)']['Experience'] < alpha:\n",
    "    print(\"There is a significant main effect of Experience.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of Experience.\")\n",
    "\n",
    "if anova_table['PR(>F)']['Programs:Experience'] < alpha:\n",
    "    print(\"There is a significant interaction effect between Programs and Experience.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between Programs and Experience.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658fbdd-e2cf-42a8-a564-ce44085b70a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e01970-1674-44e0-9450-481b4dc03870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Sample t-Test Results:\n",
      "t-statistic: -3.0031208261723967\n",
      "p-value: 0.0033913185510394315\n",
      "\n",
      "Tukey's HSD Post-Hoc Test Results:\n",
      "   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "=========================================================\n",
      " group1    group2    meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------------\n",
      "Control Experimental   5.4325 0.0034 1.8427 9.0224   True\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.stats.multicomp as mc\n",
    "\n",
    "# Generating random data for demonstration (replace with your actual data)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "control_group = np.random.normal(75, 10, 50)  # mean=75, std=10\n",
    "experimental_group = np.random.normal(78, 10, 50)  # mean=78, std=10\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_stat, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Display the t-test results\n",
    "print(\"Two-Sample t-Test Results:\")\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Follow up with post-hoc test (Tukey's HSD)\n",
    "data = np.concatenate([control_group, experimental_group])\n",
    "group_labels = ['Control'] * 50 + ['Experimental'] * 50\n",
    "\n",
    "# Create a DataFrame for post-hoc testing\n",
    "posthoc_data = pd.DataFrame({'Scores': data, 'Group': group_labels})\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "posthoc_result = mc.MultiComparison(posthoc_data['Scores'], posthoc_data['Group']).tukeyhsd()\n",
    "\n",
    "# Display post-hoc test results\n",
    "print(\"\\nTukey's HSD Post-Hoc Test Results:\")\n",
    "print(posthoc_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb8ef7-c13e-4c54-b72a-2fa80a8d374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef74ccd3-6bcb-4fb8-b13c-fff894fff762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA Results:\n",
      "F-statistic: 4.05386635503989\n",
      "p-value: 0.020734756801130193\n",
      "\n",
      "Tukey's HSD Post-Hoc Test Results:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      "group1 group2  meandiff p-adj    lower    upper   reject\n",
      "--------------------------------------------------------\n",
      "     A      B  115.8201 0.0527   -1.0582 232.6984  False\n",
      "     A      C    -9.536 0.9794 -126.4143 107.3423  False\n",
      "     B      C -125.3561 0.0326 -242.2344  -8.4778   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "import pandas as pd\n",
    "import statsmodels.stats.multicomp as mc\n",
    "\n",
    "# Generating random data for demonstration (replace with your actual data)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "sales_store_A = np.random.normal(1000, 200, 30)  # mean=1000, std=200\n",
    "sales_store_B = np.random.normal(1100, 180, 30)  # mean=1100, std=180\n",
    "sales_store_C = np.random.normal(950, 220, 30)  # mean=950, std=220\n",
    "\n",
    "# Combine data from all stores\n",
    "all_sales = np.concatenate([sales_store_A, sales_store_B, sales_store_C])\n",
    "\n",
    "# Create a grouping variable\n",
    "group_labels = ['A'] * 30 + ['B'] * 30 + ['C'] * 30\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "result_anova = f_oneway(sales_store_A, sales_store_B, sales_store_C)\n",
    "\n",
    "# Display the ANOVA results\n",
    "print(\"One-way ANOVA Results:\")\n",
    "print(\"F-statistic:\", result_anova.statistic)\n",
    "print(\"p-value:\", result_anova.pvalue)\n",
    "\n",
    "# Follow up with post-hoc test (Tukey's HSD)\n",
    "posthoc_data = pd.DataFrame({'Sales': all_sales, 'Store': group_labels})\n",
    "posthoc_result = mc.MultiComparison(posthoc_data['Sales'], posthoc_data['Store']).tukeyhsd()\n",
    "\n",
    "# Display post-hoc test results\n",
    "print(\"\\nTukey's HSD Post-Hoc Test Results:\")\n",
    "print(posthoc_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6c3b2-d540-4101-a9e7-f7bd961e7af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
